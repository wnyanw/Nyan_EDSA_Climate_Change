{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/climateedsatrain/train.csv\n",
      "/kaggle/input/climateedsatest/test.csv\n",
      "/kaggle/input/climate-edsa/test.csv\n",
      "/kaggle/input/climate-change-edsa2020-21/train.csv\n",
      "/kaggle/input/climate-change-edsa2020-21/test.csv\n",
      "/kaggle/input/climate-change-edsa2020-21/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#import libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import f1_score\n",
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load files for train and test\n",
    "train_df1 = pd.read_csv('/kaggle/input/climateedsatrain/train.csv')\n",
    "test_df1 = pd.read_csv('/kaggle/input/climateedsatest/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if file loaded\n",
    "#print(train_df1)\n",
    "#print(test_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         hashtags  counts\n",
      "0         climate     130\n",
      "6     environment      44\n",
      "11  climatechange      42\n",
      "57          Trump      25\n",
      "47           news      20\n",
      "           hashtags  counts\n",
      "19          climate     187\n",
      "24   BeforeTheFlood     129\n",
      "68    climatechange      94\n",
      "13  ImVotingBecause      62\n",
      "4             COP22      59\n",
      "          hashtags  counts\n",
      "27         climate      16\n",
      "9    climatechange      11\n",
      "104          Trump      11\n",
      "4    ClimateChange       4\n",
      "70       amreading       4\n",
      "         hashtags  counts\n",
      "30           MAGA      11\n",
      "48        climate      10\n",
      "12          Trump       7\n",
      "72  climatechange       6\n",
      "7    OpChemtrails       4\n"
     ]
    }
   ],
   "source": [
    "#make a copy\n",
    "df1 = train_df1.copy()\n",
    "#print(df1)\n",
    "df2 = train_df1.copy()\n",
    "\n",
    "df1.loc[df1['sentiment'] == -1, 'sentiment_word'] = 'Anti'\n",
    "df1.loc[df1['sentiment'] == 0, 'sentiment_word'] = 'Neutral'\n",
    "df1.loc[df1['sentiment'] == 1, 'sentiment_word'] = 'Pro'\n",
    "df1.loc[df1['sentiment'] == 2, 'sentiment_word'] = 'News'\n",
    "#print(df1.head())\n",
    "#get hashtags from messages\n",
    "#df1['hashtags'] = df1['message'].str.findall(r'#(\\w+)')\n",
    "#def findhashtag(messages):\n",
    "news_check = df1['sentiment_word'] == 'News'\n",
    "news = df1[news_check]\n",
    "pro_check = df1['sentiment_word'] == 'Pro'\n",
    "pro = df1[pro_check]\n",
    "neutral_check = df1['sentiment_word'] == 'Neutral'\n",
    "neutral = df1[neutral_check]\n",
    "anti_check = df1['sentiment_word'] == 'Anti'\n",
    "anti = df1[anti_check]\n",
    "#print(news.head())\n",
    "#print(pro.head())\n",
    "#print(neutral.head())\n",
    "#print(anti.head())\n",
    "pro_tag = []\n",
    "for i in pro['message']:\n",
    "    hashes = re.findall(r\"#(\\w+)\", i)\n",
    "    pro_tag.append(hashes)\n",
    "pro_tag = sum(pro_tag,[])\n",
    "pro_freq = nltk.FreqDist(pro_tag)\n",
    "pro_df = pd.DataFrame({'hashtags': list(pro_freq.keys()), 'counts': list(pro_freq.values())})\n",
    "pro_df = pro_df.nlargest(15, columns=\"counts\")\n",
    "#pro_df = pd.DataFrame({'hashtags': pro_tag, 'counts': list(pro_freq.values())})\n",
    "#News_tags = pd.DataFrame({'hashtags': list(freq.keys()), 'counts': list(freq.values())})\n",
    "#print(pro_df)\n",
    "\n",
    "news_tag = []\n",
    "for i in news['message']:\n",
    "    hashes = re.findall(r\"#(\\w+)\", i)\n",
    "    news_tag.append(hashes)\n",
    "news_tag = sum(news_tag,[])\n",
    "news_freq = nltk.FreqDist(news_tag)\n",
    "news_df = pd.DataFrame({'hashtags': list(news_freq.keys()), 'counts': list(news_freq.values())})\n",
    "news_df = news_df.nlargest(15, columns=\"counts\")\n",
    "\n",
    "anti_tag = []\n",
    "for i in anti['message']:\n",
    "    hashes = re.findall(r\"#(\\w+)\", i)\n",
    "    anti_tag.append(hashes)\n",
    "anti_tag = sum(anti_tag,[])\n",
    "anti_freq = nltk.FreqDist(anti_tag)\n",
    "anti_df = pd.DataFrame({'hashtags': list(anti_freq.keys()), 'counts': list(anti_freq.values())})\n",
    "anti_df = anti_df.nlargest(15, columns=\"counts\")\n",
    "\n",
    "neutral_tag = []\n",
    "for i in neutral['message']:\n",
    "    hashes = re.findall(r\"#(\\w+)\", i)\n",
    "    neutral_tag.append(hashes)\n",
    "neutral_tag = sum(neutral_tag,[])\n",
    "neutral_freq = nltk.FreqDist(neutral_tag)\n",
    "neutral_df = pd.DataFrame({'hashtags': list(neutral_freq.keys()), 'counts': list(neutral_freq.values())})\n",
    "neutral_df = neutral_df.nlargest(15, columns=\"counts\")\n",
    "\n",
    "print(news_df.head())\n",
    "print(pro_df.head())\n",
    "print(neutral_df.head())\n",
    "print(anti_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment                                            message  tweetid  \\\n",
      "0          1  polyscimajor epa chief doesn't think carbon di...   625221   \n",
      "1          1  it's not like we lack evidence of anthropogeni...   126103   \n",
      "2          2   awstory: researchers say we have three years ...   698562   \n",
      "3          1   wired :  was a pivotal year in the war on cli...   573736   \n",
      "4          1   oynoviodetodas: it's , and a racist, sexist, ...   466954   \n",
      "\n",
      "  sentiment_word  \n",
      "0            Pro  \n",
      "1            Pro  \n",
      "2           News  \n",
      "3            Pro  \n",
      "4            Pro  \n"
     ]
    }
   ],
   "source": [
    "#make the messages to lower case\n",
    "df1['message'] = df1['message'].str.lower()\n",
    "\n",
    "#df2['message'] = df2['message'].str.lower()\n",
    "#df2['message'] = df2['message'].apply(lambda x: re.sub(r'@[\\w]*', x))\n",
    "#print(df2)\n",
    "#\n",
    "#for i in df1['sentiment']:\n",
    "#    if i == 2:\n",
    "#        df1['sentiments']='News'\n",
    "#    elif i == 1:\n",
    "#        df1['sentiments']='Pro'\n",
    "#    elif i == 0:\n",
    "#        df1['sentiments']='Neutral'\n",
    "#    elif i == -1:\n",
    "#        df1['sentiments']='Anti'\n",
    "\n",
    "#print(df1.head(20))\n",
    "\n",
    "#lemma words in message\n",
    "\n",
    "\n",
    "#extract hastag using re\n",
    "#add hashtag column\n",
    "#df1['hashtags'] = df1['message'].str.findall(r'#.*?(?=\\s|$)')\n",
    "#df1['hashtags'] = df1['message'].str.findall(r'#(\\w+)')\n",
    "#df2 = df1.dropna()\n",
    "#print(type(df1['hashtags']))\n",
    "#print(df1)\n",
    "\n",
    "#df1['hashtags'] = df1['hashtags'].to_string()\n",
    "#df1['lowerHashtags'] = df1['hashtags'].str.lower()\n",
    "#print(type(df1['message']))\n",
    "#print(df1)\n",
    "#df1\n",
    "#df1['firstHashtags'] = df1['hashtags'].iloc[0]\n",
    "#df1.astype(str)\n",
    "#df1.dtypes\n",
    "###################\n",
    "#hashtags_only = []\n",
    "#tweetMsg = df1['message']\n",
    "#for i in tweetMsg:\n",
    "#    hashtag = re.findall(r'#(\\w+)', i)\n",
    "#    hashtags_only.append(hashtag)\n",
    "#hasht = sum(hashtags_only,[])\n",
    "#freq = nltk.FreqDist(hasht)\n",
    "#df1_hashtags = pd.DataFrame({'hashtag': list(freq.keys()), 'counts': list(freq.values())})\n",
    "#print(df1_hashtags)\n",
    "##########################\n",
    "#hashtag only dataframe\n",
    "#print(df1)\n",
    "#hashtag_df1['hashtags'] = df1.explode('hashtags')\n",
    "#hashtag_df1\n",
    "#print(hashtag_df1)\n",
    "#hashOnly['hashtags'] = hashtag_df1['hashtags']\n",
    "#print(hashonly)\n",
    "#hashOnly = hashOnly.dropna()\n",
    "#print(hashonly)\n",
    "#hashOnlyUnq = hashOnly.sort_values()\n",
    "#print(hashOnlyUnq)\n",
    "#hashDup = hashOnlyUnq.pivot_table(index=['hashtags'], addfunc='size')\n",
    "#\n",
    "#print(df1)\n",
    "\n",
    "def preprocess(message):\n",
    "    message = re.sub(r\"#\\w*\", '', message)\n",
    "    message = re.sub(r\"\\d+\", '', message)\n",
    "    message = re.sub(r\"@[A-Za-z0-9]\", '', message)\n",
    "    message = re.sub(r\"rt[\\s]+\", ' ', message)\n",
    "    message = re.sub(r\"https?:\\/\\/\\S+\", ' ', message)\n",
    "    return message\n",
    "df1['message'] = df1['message'].apply(preprocess)\n",
    "print(df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>sentiment_word</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>len</th>\n",
       "      <th>speechTags</th>\n",
       "      <th>text_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>polyscimajor epa chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "      <td>Pro</td>\n",
       "      <td>[polyscimajor, epa, chief, does, n't, think, c...</td>\n",
       "      <td>116</td>\n",
       "      <td>[(polyscimajor, JJ), (epa, NN), (chief, NN), (...</td>\n",
       "      <td>[polyscimajor, epa, chief, doesn't, think, car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>it's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "      <td>Pro</td>\n",
       "      <td>[it, 's, not, like, we, lack, evidence, of, an...</td>\n",
       "      <td>62</td>\n",
       "      <td>[(it, PRP), ('s, VBZ), (not, RB), (like, IN), ...</td>\n",
       "      <td>[it's, not, like, we, lack, evidence, of, anth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>awstory: researchers say we have three years ...</td>\n",
       "      <td>698562</td>\n",
       "      <td>News</td>\n",
       "      <td>[awstory, :, researchers, say, we, have, three...</td>\n",
       "      <td>95</td>\n",
       "      <td>[(awstory, NN), (:, :), (researchers, NNS), (s...</td>\n",
       "      <td>[awstory:, researcher, say, we, have, three, y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>wired :  was a pivotal year in the war on cli...</td>\n",
       "      <td>573736</td>\n",
       "      <td>Pro</td>\n",
       "      <td>[wired, :, was, a, pivotal, year, in, the, war...</td>\n",
       "      <td>59</td>\n",
       "      <td>[(wired, VBN), (:, :), (was, VBD), (a, DT), (p...</td>\n",
       "      <td>[wired, :, wa, a, pivotal, year, in, the, war,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>oynoviodetodas: it's , and a racist, sexist, ...</td>\n",
       "      <td>466954</td>\n",
       "      <td>Pro</td>\n",
       "      <td>[oynoviodetodas, :, it, 's, ,, and, a, racist,...</td>\n",
       "      <td>100</td>\n",
       "      <td>[(oynoviodetodas, NNS), (:, :), (it, PRP), ('s...</td>\n",
       "      <td>[oynoviodetodas:, it's, ,, and, a, racist,, se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid  \\\n",
       "0          1  polyscimajor epa chief doesn't think carbon di...   625221   \n",
       "1          1  it's not like we lack evidence of anthropogeni...   126103   \n",
       "2          2   awstory: researchers say we have three years ...   698562   \n",
       "3          1   wired :  was a pivotal year in the war on cli...   573736   \n",
       "4          1   oynoviodetodas: it's , and a racist, sexist, ...   466954   \n",
       "\n",
       "  sentiment_word                                          tokenized  len  \\\n",
       "0            Pro  [polyscimajor, epa, chief, does, n't, think, c...  116   \n",
       "1            Pro  [it, 's, not, like, we, lack, evidence, of, an...   62   \n",
       "2           News  [awstory, :, researchers, say, we, have, three...   95   \n",
       "3            Pro  [wired, :, was, a, pivotal, year, in, the, war...   59   \n",
       "4            Pro  [oynoviodetodas, :, it, 's, ,, and, a, racist,...  100   \n",
       "\n",
       "                                          speechTags  \\\n",
       "0  [(polyscimajor, JJ), (epa, NN), (chief, NN), (...   \n",
       "1  [(it, PRP), ('s, VBZ), (not, RB), (like, IN), ...   \n",
       "2  [(awstory, NN), (:, :), (researchers, NNS), (s...   \n",
       "3  [(wired, VBN), (:, :), (was, VBD), (a, DT), (p...   \n",
       "4  [(oynoviodetodas, NNS), (:, :), (it, PRP), ('s...   \n",
       "\n",
       "                                          text_lemma  \n",
       "0  [polyscimajor, epa, chief, doesn't, think, car...  \n",
       "1  [it's, not, like, we, lack, evidence, of, anth...  \n",
       "2  [awstory:, researcher, say, we, have, three, y...  \n",
       "3  [wired, :, wa, a, pivotal, year, in, the, war,...  \n",
       "4  [oynoviodetodas:, it's, ,, and, a, racist,, se...  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenize, lemmanize\n",
    "df1.head()\n",
    "word_tokens = []\n",
    "#for i in df1['message']:\n",
    "#    word_tokens.append(word_tokenize(i))\n",
    "df1['tokenized'] = df1['message'].apply(word_tokenize)\n",
    "df1['len'] = df1['message'].str.len()\n",
    "df1['speechTags'] = df1['tokenized'].apply(nltk.tag.pos_tag)\n",
    "#df1['lemmanize'] = df1['message'].apply(WordNetLemmatizer)\n",
    "w_token = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmanizetext(text):\n",
    "    return [lemmatizer.lemmatize(x) for x in w_token.tokenize(text)]\n",
    "df1['text_lemma'] = df1.message.apply(lemmanizetext)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df1['message']\n",
    "y = train_df1['sentiment']\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9059     1\n",
       "169      2\n",
       "5331     1\n",
       "6333    -1\n",
       "10383    0\n",
       "        ..\n",
       "1950     0\n",
       "14723    1\n",
       "651      2\n",
       "5015     2\n",
       "7567     2\n",
       "Name: sentiment, Length: 3955, dtype: int64"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pipeline can be used as any other estimator\n",
    "# and avoids leaking the test set into the train set\n",
    "pipe = Pipeline\n",
    "logreg = pipe([('vect',TfidfVectorizer()),\n",
    "               ('logreg',LogisticRegression(C=1, max_iter=10000))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_valid)\n",
    "y_test = logreg.predict(test_df1['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetid</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Europe will now be looking to China to make su...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Combine this with the polling of staffers re c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The scary, unimpeachable evidence that climate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Karoli @morgfair @OsborneInk @dailykos \\nPuti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @FakeWillMoore: 'Female orgasms cause globa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10541</th>\n",
       "      <td>RT @BrittanyBohrer: Brb, writing a poem about ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10542</th>\n",
       "      <td>2016: the year climate change came home: Durin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10543</th>\n",
       "      <td>RT @loop_vanuatu: Pacific countries positive a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10544</th>\n",
       "      <td>RT @xanria_00018: You’re so hot, you must be t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10545</th>\n",
       "      <td>RT @chloebalaoing: climate change is a global ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10546 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweetid  sentiment\n",
       "0      Europe will now be looking to China to make su...          1\n",
       "1      Combine this with the polling of staffers re c...          1\n",
       "2      The scary, unimpeachable evidence that climate...          1\n",
       "3      @Karoli @morgfair @OsborneInk @dailykos \\nPuti...          1\n",
       "4      RT @FakeWillMoore: 'Female orgasms cause globa...          0\n",
       "...                                                  ...        ...\n",
       "10541  RT @BrittanyBohrer: Brb, writing a poem about ...          1\n",
       "10542  2016: the year climate change came home: Durin...          1\n",
       "10543  RT @loop_vanuatu: Pacific countries positive a...          1\n",
       "10544  RT @xanria_00018: You’re so hot, you must be t...          0\n",
       "10545  RT @chloebalaoing: climate change is a global ...          1\n",
       "\n",
       "[10546 rows x 2 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame({'tweetid': test_df1['tweetid'], 'sentiment': y_test})\n",
    "#submission.to_csv('/kaggle/input/climateedsatrain/submission.csv', index = False)\n",
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
